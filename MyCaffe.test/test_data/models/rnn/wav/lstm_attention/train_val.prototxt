name: "Char-RNN"
layer 
{
   name: "Input"
   type: "Input"
   top: "data"
   top: "clip"
   top: "label"
   input_param 
   {
      shape 
      {
         dim: 75 #data sequence_length
         dim: 1  
      }
      shape 
      {
         dim: 75 #clip sequence_length
         dim: 1
      }
      shape 
      {
         dim: 75 #label sequence_length
         dim: 1
      }
   }
}
layer 
{
   name: "EmbedLayer"
   type: "Embed"
   bottom: "data"
   top: "inputVectors"
   embed_param 
   {
      num_output: 15
      input_dim: 128 #vocabulary size (dynamically resized to actual vocabulary size)
      weight_filler 
      {
         type: "xavier"
         variance_norm: FAN_IN
      }
      bias_filler 
      {
         type: "constant"
         value: 0.1
      }
   }
}
layer 
{
   name: "lstm1"
   type: "LstmAttention"
   bottom: "inputVectors"
   bottom: "clip"
   top: "lstm1"
   lstm_attention_param 
   {
      num_output: 512
      weight_filler 
      {
         type: "uniform"
         min: -0.01
         max: 0.01
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
   }
}
layer 
{
   name: "Drop1"
   type: "Dropout"
   bottom: "lstm1"
   top: "lstm1_drop"
   dropout_param 
   {
      dropout_ratio: 0.4
   }
}
layer 
{
   name: "lstm2"
   type: "LstmAttention"
   bottom: "lstm1_drop"
   bottom: "clip"
   top: "lstm2"
   lstm_attention_param 
   {
      num_output: 512
      weight_filler 
      {
         type: "uniform"
         min: -0.01
         max: 0.01
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
   }
}
layer 
{
   name: "Drop2"
   type: "Dropout"
   bottom: "lstm2"
   top: "lstm2_drop"
   dropout_param 
   {
      dropout_ratio: 0.4
   }
}
layer 
{
   name: "Reshape_lstm2"
   type: "Reshape"
   bottom: "lstm2_drop"
   top: "lstm2_reshaped"
   reshape_param 
   {
      shape 
      {
         dim: -1
         dim: 1
         dim: 512
      }
   }
}
layer 
{
   name: "ip1"
   type: "InnerProduct"
   bottom: "lstm2_reshaped"
   top: "ip1"
   inner_product_param 
   {
      num_output: 128 #vocabulary size (dynamically resized to actual vocabulary size)
      bias_term: True
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.1
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
      axis: 1
   }
}
layer 
{
   name: "Loss"
   type: "SoftmaxWithLoss"
   bottom: "ip1"
   bottom: "label"
   top: "loss"
   exclude 
   {
      phase: RUN
   }
   loss_param 
   {
      normalization: VALID
   }
}
