name: "BasicNet"
layer 
{
   name: "tokdata1"
   type: "TokenizedData"
   top: "tokdata1"
   top: "pos"
   top: "tgt"
   tokenized_data_param 
   {
      input_type: TEXT_FILE
      source: "C:\ProgramData\MyCaffe\test_data\data\text\input.txt"
      batch_size: 1
      block_size: 128
   }
}
layer 
{
   name: "wte"
   type: "Embed"
   bottom: "tokdata1"
   top: "tok_emb"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   embed_param 
   {
      num_output: 192
      input_dim: 65
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
   }
}
layer 
{
   name: "wpe"
   type: "Embed"
   bottom: "pos"
   top: "pos_emb"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   embed_param 
   {
      num_output: 192
      input_dim: 128
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
   }
}
layer 
{
   name: "eltwise1"
   type: "Eltwise"
   bottom: "tok_emb"
   bottom: "pos_emb"
   top: "eltwise1"
   eltwise_param 
   {
      operation: SUM
   }
}
layer 
{
   name: "dropout1"
   type: "Dropout"
   bottom: "eltwise1"
   top: "eltwise1"
   dropout_param 
   {
      dropout_ratio: 0.1
   }
}
layer 
{
   name: "tfb1"
   type: "TransformerBlock"
   bottom: "eltwise1"
   top: "tfb1"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb2"
   type: "TransformerBlock"
   bottom: "tfb1"
   top: "tfb2"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb3"
   type: "TransformerBlock"
   bottom: "tfb2"
   top: "tfb3"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb4"
   type: "TransformerBlock"
   bottom: "tfb3"
   top: "tfb4"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb5"
   type: "TransformerBlock"
   bottom: "tfb4"
   top: "tfb5"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb6"
   type: "TransformerBlock"
   bottom: "tfb5"
   top: "tfb6"
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "ln1"
   type: "LayerNorm"
   bottom: "tfb6"
   top: "ln1"
}
layer 
{
   name: "ip1"
   type: "InnerProduct"
   bottom: "ln1"
   top: "logit"
   param 
   {
      lr_mult: 1
      decay_mult: 2
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 65
      bias_term: True
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      bias_filler 
      {
         type: "constant"
         value: 0
      }
      axis: 2
   }
}
layer 
{
   name: "loss1"
   type: "SoftmaxCrossEntropyLoss"
   bottom: "logit"
   bottom: "tgt"
   top: "loss1"
   loss_param 
   {
      normalization: BATCH_SIZE
   }
}
layer 
{
   name: "accuracy1"
   type: "Accuracy"
   bottom: "logit"
   bottom: "tgt"
   top: "accuracy1"
   include 
   {
      phase: TEST
   }
}
