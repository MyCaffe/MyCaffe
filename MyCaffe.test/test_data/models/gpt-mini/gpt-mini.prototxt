name: "GptMiniNet"
layer 
{
   name: "tokdata1"
   type: "TokenizedData"   
   top: "tokdata"
   top: "pos"
   top: "tgt"
   tokenized_data_param 
   {
      input_type: TEXT_FILE
      source: "$ProgramData$\MyCaffe\test_data\data\text\input.txt"
      batch_size: 64
      block_size: 128
   }
}
layer 
{
   name: "wte"
   type: "Embed"
   bottom: "tokdata"
   top: "tok_emb"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   embed_param 
   {
      num_output: 192
      input_dim: 65
      bias_term: False
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      bias_filler
      {
         type: "constant"
         value: 0.1
      }
   }
}
layer 
{
   name: "wpe"
   type: "Embed"
   bottom: "pos"
   top: "pos_emb"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   embed_param 
   {
      num_output: 192
      input_dim: 128
      bias_term: False
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      bias_filler
      {
         type: "constant"
         value: 0.1
      }
   }
}
layer 
{
   name: "eltwise1"
   type: "Eltwise"
   bottom: "tok_emb"
   bottom: "pos_emb"
   top: "eltwise1"
   eltwise_param 
   {
      operation: SUM
      allow_single_batch_input: True
   }
}
layer 
{
   name: "dropout1"
   type: "Dropout"
   bottom: "eltwise1"
   top: "eltwise1"
   dropout_param 
   {
      dropout_ratio: 0.1
   }
}
layer 
{
   name: "tfb1"
   type: "TransformerBlock"
   bottom: "eltwise1"
   top: "tfb1"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb2"
   type: "TransformerBlock"
   bottom: "tfb1"
   top: "tfb2"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb3"
   type: "TransformerBlock"
   bottom: "tfb2"
   top: "tfb3"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb4"
   type: "TransformerBlock"
   bottom: "tfb3"
   top: "tfb4"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb5"
   type: "TransformerBlock"
   bottom: "tfb4"
   top: "tfb5"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "tfb6"
   type: "TransformerBlock"
   bottom: "tfb5"
   top: "tfb6"
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   param
   {
      lr_mult: 1
      decay_mult: 1
   }
   param
   {
      lr_mult: 1
      decay_mult: 0
   }
   transformer_block_param 
   {
      layers: 6
      heads: 6
      embed: 192
      block_size: 128
      attn_dropout: 0.1
      resid_dropout: 0.1
   }
}
layer 
{
   name: "ln1"
   type: "LayerNorm"
   bottom: "tfb6"
   top: "ln1"
}
layer 
{
   name: "ip1"
   type: "InnerProduct"
   bottom: "ln1"
   top: "logits"
   param 
   {
      lr_mult: 1
      decay_mult: 1
   }
   inner_product_param 
   {
      num_output: 65
      bias_term: False
      weight_filler 
      {
         type: "gaussian"
         mean: 0
         std: 0.02
      }
      axis: 2
   }
}
layer 
{
   name: "loss1"
   type: "SoftmaxCrossEntropy2Loss"
   bottom: "logits"
   bottom: "tgt"
   top: "loss1"
   loss_param 
   {
      normalization: BATCH_SIZE
   }
   softmax_param 
   {
      axis: 2
   }
}
layer 
{
   name: "accuracy1"
   type: "Accuracy"
   bottom: "logits"
   bottom: "tgt"
   top: "accuracy1"
   include 
   {
      phase: TEST
   }
   accuracy_param 
   {
      axis: 2
   }
}
