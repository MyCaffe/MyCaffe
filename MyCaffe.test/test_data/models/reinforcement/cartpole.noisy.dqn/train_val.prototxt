name: "BasicNet"
layer 
{
   name: "input"
   type: "Input"
   top: "data"
   input_param 
   {
      shape 
      {
         dim: 32
         dim: 4
         dim: 1
         dim: 1
      }
   }
}
layer 
{
   name: "linear"
   type: "InnerProduct"
   bottom: "data"
   top: "linear"
   param 
   {
      lr_mult: 1
      decay_mult: 2
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 128
      bias_term: True
      weight_filler 
      {
         type: "xavier"
         variance_norm: FAN_IN
      }
      bias_filler 
      {
         type: "constant"
         value: 0.1
      }
      axis: 1
   }
}
layer 
{
   name: "relu1"
   type: "PReLU"
   bottom: "linear"
   top: "linear"
   prelu_param 
   {
      filler 
      {
         type: "constant"
         value: 0.25
      }
   }
}
layer 
{
   name: "noisy1"
   type: "InnerProduct"
   bottom: "linear"
   top: "noisy1"
   param 
   {
      lr_mult: 1
      decay_mult: 2
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 128
      bias_term: True
      weight_filler 
      {
         type: "xavier"
         variance_norm: FAN_IN
      }
      bias_filler 
      {
         type: "constant"
         value: 0.1
      }
      axis: 1
      enable_noise: True
      sigma_init: 0.4
   }
}
layer 
{
   name: "relu2"
   type: "PReLU"
   bottom: "noisy1"
   top: "noisy1"
   prelu_param 
   {
      filler 
      {
         type: "constant"
         value: 0.25
      }
   }
}
layer 
{
   name: "noisy2"
   type: "InnerProduct"
   bottom: "noisy1"
   top: "logits"
   param 
   {
      lr_mult: 1
      decay_mult: 2
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 2
      bias_term: True
      weight_filler 
      {
         type: "xavier"
         variance_norm: FAN_IN
      }
      bias_filler 
      {
         type: "constant"
         value: 0.1
      }
      axis: 1
      enable_noise: True
      sigma_init: 0.4
   }
}
layer 
{
   name: "Softmax1"
   type: "Softmax"
   bottom: "logits"
   top: "logits"
}
layer 
{
   name: "loss1"
   type: "MemoryLoss"
   bottom: "logits"
   top: "loss1"
   include 
   {
      phase: TRAIN
   }
   loss_param 
   {
      normalization: NONE
   }
}
