name: "BasicNet"
layer 
{
   name: "input"
   type: "Input"
   top: "data"
   input_param 
   {
      shape 
      {
         dim: 32
         dim: 4
         dim: 1
         dim: 1
      }
   }
}
layer 
{
   name: "linear"
   type: "InnerProduct"
   bottom: "data"
   top: "linear"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 128
      bias_term: True
      weight_filler 
      {
         type: "uniform"
		 min: -1
		 max: 1
      }
      bias_filler 
      {
         type: "uniform"
		 min: -1
		 max: 1
      }
      axis: 1
   }
}
layer 
{
   name: "relu1"
   type: "ReLU"
   bottom: "linear"
   top: "linear"
}
layer 
{
   name: "noisy1"
   type: "InnerProduct"
   bottom: "linear"
   top: "noisy1"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 128
      bias_term: True
      weight_filler 
      {
         type: "uniform"
		 min: -1
		 max: 1
      }
      bias_filler 
      {
         type: "uniform"
		 min: -1
		 max: 1
      }
      axis: 1
      enable_noise: True
      sigma_init: 0.17
   }
}
layer 
{
   name: "relu2"
   type: "ReLU"
   bottom: "noisy1"
   top: "noisy1"
}
layer 
{
   name: "noisy2"
   type: "InnerProduct"
   bottom: "noisy1"
   top: "logits"
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   param 
   {
      lr_mult: 1
      decay_mult: 0
   }
   inner_product_param 
   {
      num_output: 2
      bias_term: True
      weight_filler 
      {
         type: "uniform"
		 min: -0.3
		 max: 0.3
      }
      bias_filler 
      {
         type: "uniform"
		 min: -0.3
		 max: 0.3
      }
      axis: 1
      enable_noise: True
      sigma_init: 0.17
   }
}
layer 
{
   name: "loss1"
   type: "MemoryLoss"
   bottom: "logits"
   top: "loss1"
   include 
   {
      phase: TRAIN
   }
   loss_param 
   {
      normalization: NONE
   }
}
